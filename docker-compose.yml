version: "3.9"

networks:
  dataplane:

volumes:
  minio_data:
  clickhouse_data:
  postgres_data:
  nessie_data:
  superset_db:
  airflow_logs:
  superset_data:

services:
  # ─── Zookeeper + Kafka ─────────────────────────────────────────
  zookeeper:
    image: confluentinc/cp-zookeeper:7.6.0
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    networks: [dataplane]
    ports: ["2181:2181"]

  kafka:
    image: confluentinc/cp-kafka:7.6.0
    depends_on: [zookeeper]
    networks: [dataplane]
    ports:
      - "9092:9092"           # internal
      - "9093:9093"           # external / host
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_LISTENERS: >
        PLAINTEXT://0.0.0.0:9092,
        PLAINTEXT_HOST://0.0.0.0:9093
        
        # advertise internal name for containers, localhost for your laptop
      KAFKA_ADVERTISED_LISTENERS: >
        PLAINTEXT://kafka:9092,
        PLAINTEXT_HOST://localhost:9093

      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_HOST:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1

  # ─── MinIO (object store) ─────────────────────────────────────
  minio:
    image: minio/minio:latest       # <— stable tag on Hub
    command: server /data --console-address ":9001"
    networks: [ dataplane ]
    ports:
      - "9000:9000"   # S3 API
      - "9001:9001"   # Console UI
    environment:
      MINIO_ROOT_USER: minio
      MINIO_ROOT_PASSWORD: minio123
    volumes:
      - minio_data:/data


  # ─── Nessie catalog (Iceberg metadata) ────────────────────────
  nessie:
    image: ghcr.io/projectnessie/nessie:0.104.2-java
    networks: [dataplane]
    ports: ["19120:19120"]
    environment:
      QUARKUS_HTTP_PORT: 19120
      NESSIE_VERSION_STORE_TYPE: ROCKSDB
    volumes:
      - nessie_data:/opt/nessie/store

  # ─── Trino (SQL federation + Iceberg catalog) ────────────────
  trino:
    image: trinodb/trino:447
    ports: [ "8081:8080" ]
    networks: [ dataplane ]
    volumes:
      - ./configs/trino/catalog:/etc/trino/catalog:ro
    depends_on: [ minio, nessie ]


  # ─── Spark (master + worker) ─────────────────────────────────
  spark-master:
    image: bitnami/spark:3.5.0
    networks: [dataplane]
    environment:
      SPARK_MODE: master
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"
    ports:
      - "7077:7077"
      - "8082:8080"
    volumes:
      - ./configs/spark/spark-defaults.conf:/opt/bitnami/spark/conf/spark-defaults.conf:ro
  spark-worker:
    image: bitnami/spark:3.5.0
    depends_on: [spark-master]
    networks: [dataplane]
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_RPC_AUTHENTICATION_ENABLED: "no"

  # ─── ClickHouse (optional low-latency OLAP) ──────────────────
  clickhouse:
    image: clickhouse/clickhouse-server:24.3.2
    networks: [dataplane]
    ports:
      - "8123:8123"
      - "9004:9000"
    volumes:
      - clickhouse_data:/var/lib/clickhouse

  # ─── Superset (BI) ───────────────────────────────────────────
  superset:
    image: apache/superset:3.0.2
    networks: [ dataplane ]
    ports:
      - "8088:8088"
    environment:
      SUPERSET_SECRET_KEY: "TH1sIsASecret_ChangeMe"
      DATABASE_URL: sqlite:////var/lib/superset/superset.db
    volumes:
      - superset_data:/var/lib/superset
    command: >
      bash -e -c "
        superset db upgrade &&
        superset fab create-admin --username admin --firstname Uzair --lastname User --email admin@example.com --password admin || true &&
        superset init &&
        superset run -p 8088 -h 0.0.0.0
      "
    depends_on: [ trino ]



  # ─── Airflow (webserver + scheduler) ─────────────────────────
  postgres:
    image: postgres:13
    networks: [dataplane]
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - postgres_data:/var/lib/postgresql/data

  airflow-init:
    image: apache/airflow:2.9.3-python3.11
    depends_on: [ postgres ]
    networks: [ dataplane ]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    entrypoint: >
      bash -e -c "airflow db upgrade &&
                  airflow users create --username admin --password admin --firstname Admin --lastname User --role Admin --email admin@example.com"
    restart: "no"


    ### ─── webserver ─────────────────
  airflow-webserver:
    image: apache/airflow:2.9.3-python3.11
    depends_on: [ airflow-init ]
    networks: [ dataplane ]
    ports: [ "8080:8080" ]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor
      AIRFLOW__CORE__LOAD_EXAMPLES: "False"
    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: webserver

  ### ─── scheduler ─────────────────
  airflow-scheduler:
    image: apache/airflow:2.9.3-python3.11
    depends_on: [ airflow-init ]
    networks: [ dataplane ]
    environment:
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@postgres/airflow
      AIRFLOW__CORE__EXECUTOR: LocalExecutor

    volumes:
      - ./airflow/dags:/opt/airflow/dags
      - airflow_logs:/opt/airflow/logs
    command: scheduler




